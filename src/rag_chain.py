# 3_rag_chain.py
"""
Agent CTI avec retrieval intelligent et Phi-3.5
"""

import re
from langchain_community.llms import Ollama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser


def get_llm():
    """Phi-3.5 via Ollama."""
    return Ollama(
        model="phi3.5",
        temperature=0.1,
        num_ctx=4096,
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# VALIDATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def is_relevant_question(question):
    """Rejette les questions hors-sujet CTI."""
    question_lower = question.lower().strip()

    irrelevant_patterns = [
        r'^(hi|hello|hey|bonjour|salut|coucou)',
        r'^how are you',
        r'^(merci|thanks|thank you)',
        r'^(bye|goodbye|au revoir)',
        r'^(oui|non|yes|no|ok|okay)$',
        r'^(test|testing)$',
        r'^what is the weather',
        r'^who are you',
        r'^what can you do',
    ]

    for pattern in irrelevant_patterns:
        if re.match(pattern, question_lower):
            return False

    if len(question_lower.split()) < 3:
        return False

    return True


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# RETRIEVER INTELLIGENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Seuil de pertinence : au-dessus = non pertinent
RELEVANCE_THRESHOLD = 0.75


def retrieve_with_context(vectorstore, query, k=10):
    """
    Recherche en 2 Ã©tapes avec FILTRAGE par score.
    """
    # Ã‰tape 1 : Posts principaux
    posts = vectorstore.similarity_search_with_score(
        query=query,
        k=k,
        filter={"doc_type": "original_post"},
    )

    results = []
    seen = set()

    for doc, score in posts:
        # FILTRAGE : ignorer les rÃ©sultats non pertinents
        if score > RELEVANCE_THRESHOLD:
            continue

        post_id = doc.metadata.get("post_id", "")

        entry = {
            "post": doc,
            "score": float(score),
            "post_id": post_id,
            "replies": [],
        }

        # Ã‰tape 2 : Replies de ce post
        if post_id and post_id not in seen:
            try:
                replies = vectorstore.similarity_search(
                    query=query,
                    k=5,
                    filter={"parent_post_id": str(post_id)},
                )
                entry["replies"] = replies
            except Exception:
                pass
            seen.add(post_id)

        results.append(entry)

    return results


def format_context(results, max_results=5):
    """
    Formate pour le prompt du LLM.
    Reconstruit le contexte depuis metadata.
    """
    if not results:
        return "AUCUN RÃ‰SULTAT PERTINENT TROUVÃ‰."

    blocks = []

    for i, r in enumerate(results[:max_results]):
        meta = r["post"].metadata
        content = r["post"].page_content
        post_id = meta.get("post_id", "?")
        channel = meta.get("channel_name", "?")

        block = f"â•â• SOURCE {i+1} "
        block += f"(score: {r['score']:.3f}) â•â•\n"
        block += (
            f"[POST_ID: {post_id}] | "
            f"CHANNEL: {channel} | "
            f"CONTENT: {content}\n"
        )

        views = meta.get("views", "")
        forwards = meta.get("forwards", "")
        if views:
            block += (
                f"  [Views: {views} | "
                f"Forwards: {forwards}]\n"
            )

        if r["replies"]:
            block += "  â”€â”€ RÃ©actions communautÃ© â”€â”€\n"
            for reply in r["replies"]:
                r_meta = reply.metadata
                r_id = r_meta.get("reply_id", "?")
                block += (
                    f"  â†’ [REPLY_ID: {r_id}] "
                    f"{reply.page_content}\n"
                )

        blocks.append(block)

    return "\n\n".join(blocks)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PROMPTS CTI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REWRITE_PROMPT = ChatPromptTemplate.from_messages([
    ("system",
     "Tu es un analyste CTI. Reformule cette question en "
     "requÃªte optimisÃ©e pour la recherche dans une base de "
     "posts Telegram cybercriminels. Ajoute des termes "
     "techniques CTI. Max 25 mots. RÃ©ponds UNIQUEMENT "
     "avec la requÃªte reformulÃ©e, rien d'autre."),
    ("human", "{question}")
])

ANALYSIS_PROMPT = ChatPromptTemplate.from_messages([
    ("system", """Tu es un analyste senior CTI.
Analyse les donnÃ©es suivantes extraites de canaux
Telegram cybercriminels (dataset DarkGram).

DONNÃ‰ES RÃ‰CUPÃ‰RÃ‰ES :
{context}

RÃˆGLES STRICTES :
1. Base-toi UNIQUEMENT sur les donnÃ©es ci-dessus
2. Ne JAMAIS inventer des informations absentes du contexte
3. Cite les POST_ID et CHANNEL exacts dans tes sources
4. Si le contexte dit "AUCUN RÃ‰SULTAT PERTINENT",
   rÃ©ponds que tu n'as pas trouvÃ© de donnÃ©es pertinentes
5. Ã‰value la fiabilitÃ© :
   - Posts avec replies interrogatives = probable spam/scam
   - Posts avec views Ã©levÃ©es + forwards = menace potentielle
   - Score de pertinence < 0.3 = trÃ¨s pertinent
   - Score de pertinence > 0.5 = peu pertinent
6. Ne fais PAS d'analyse si les donnÃ©es sont insuffisantes

FORMAT :
## Analyse
[Ton analyse factuelle basÃ©e sur les donnÃ©es]

## Indicateurs de Menace (IOC)
- [UNIQUEMENT ceux prÃ©sents dans le contexte]

## Sources
- POST_ID: X | Channel: Y | Score: Z

## FiabilitÃ© Globale
[Ã‰levÃ©e/Moyenne/Faible] - [justification]"""),
    ("human", "{question}")
])


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# AGENT CTI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class CTIAgent:
    def __init__(self, vectorstore):
        self.vectorstore = vectorstore
        self.llm = get_llm()
        self.parser = StrOutputParser()
        self.rewrite_chain = (
            REWRITE_PROMPT | self.llm | self.parser
        )
        self.analysis_chain = (
            ANALYSIS_PROMPT | self.llm | self.parser
        )

    def analyze(self, question, k=10, verbose=True):
        """Pipeline RAG complet avec validation."""

        # VÃ©rification pertinence
        if not is_relevant_question(question):
            msg = (
                "âš ï¸ Cette question ne semble pas liÃ©e "
                "Ã  la Cyber Threat Intelligence.\n\n"
                "Exemples de questions valides :\n"
                "- What cracking tools are shared?\n"
                "- Which channels sell stolen credentials?\n"
                "- What cloud logs are available?\n"
                "- Are there pirated software shared?"
            )
            if verbose:
                print(f"\nâš ï¸ Question hors-sujet dÃ©tectÃ©e")
            return {
                "question": question,
                "rewritten": None,
                "analysis": msg,
                "sources": [],
            }

        # 1. Reformulation
        if verbose:
            print(f"\nğŸ” Question : {question}")

        rewritten = self.rewrite_chain.invoke(
            {"question": question}
        )
        if verbose:
            print(f"ğŸ”„ ReformulÃ©e : {rewritten}")

        # 2. Retrieval
        results = retrieve_with_context(
            self.vectorstore, rewritten, k=k
        )
        if verbose:
            print(f"ğŸ“¦ {len(results)} rÃ©sultats pertinents")

        # Aucun rÃ©sultat pertinent
        if not results:
            if verbose:
                print("âŒ Aucun rÃ©sultat sous le seuil")
            return {
                "question": question,
                "rewritten": rewritten,
                "analysis": (
                    "âŒ Aucun rÃ©sultat pertinent trouvÃ© "
                    "dans la base DarkGram.\n"
                    "Les documents trouvÃ©s avaient un score "
                    "de similaritÃ© trop faible "
                    f"(seuil: {RELEVANCE_THRESHOLD})."
                ),
                "sources": [],
            }

        # 3. Formatage
        context = format_context(results, max_results=5)
        if verbose:
            print(f"ğŸ“‹ Contexte : {len(context)} car.")

        # 4. Analyse
        if verbose:
            print(f"ğŸ¤– Analyse en cours...")

        analysis = self.analysis_chain.invoke({
            "context": context,
            "question": question,
        })

        return {
            "question": question,
            "rewritten": rewritten,
            "analysis": analysis,
            "sources": [
                {
                    "post_id": r["post_id"],
                    "score": r["score"],
                    "replies": len(r["replies"]),
                    "channel": r["post"].metadata.get(
                        "channel_name", ""
                    ),
                }
                for r in results[:5]
            ],
        }