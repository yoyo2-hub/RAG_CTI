import json
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import re
from collections import Counter
from pathlib import Path

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 1 : CHARGEMENT ROBUSTE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

filepath = Path('JSONL\darkgram_cti_final.jsonl')  
data = []
errors = 0

print("ğŸ” Chargement du fichier JSONL...")
try:
    with open(filepath, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            try:
                obj = json.loads(line.strip())
                if 'text' in obj:  # Validation minimale
                    data.append(obj)
                else:
                    errors += 1
            except json.JSONDecodeError:
                errors += 1
    print(f"âœ… {len(data)} documents chargÃ©s. {errors} lignes ignorÃ©es.")
except FileNotFoundError:
    print(f"âŒ Fichier introuvable : {filepath}")
    exit()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 2 : PARSING INTELLIGENT DU FORMAT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def parse_doc_type(text, metadata):
    """Parse le type DEPUIS le texte structurÃ© (source de vÃ©ritÃ©)"""
    # Le type est encodÃ© dans le texte : "TYPE: MAIN_POST" ou "TYPE: REPLY"
    match = re.search(r'TYPE:\s*(\w+)', text)
    if match:
        raw = match.group(1).upper()
        if raw in ('REPLY',):
            return 'reply'
        elif raw in ('MAIN_POST',):
            return 'original_post'

    # Fallback sur la structure des mÃ©tadonnÃ©es
    if 'parent_post_id' in metadata:
        return 'reply'
    return 'original_post'


def extract_content(text):
    """Extrait le CONTENU rÃ©el en supprimant les mÃ©tadonnÃ©es structurelles"""
    # Supprime les tags structurels : [POST_ID: X] | TYPE: Y | CHANNEL: Z |
    content_match = re.search(r'CONTENT:\s*(.+?)(?:\s*\|\s*Statut:|$)', text, re.DOTALL)
    if content_match:
        return content_match.group(1).strip()
    # Si pas de tag CONTENT, retourne le texte brut nettoyÃ©
    cleaned = re.sub(
        r'\[(?:POST_ID|PARENT_POST_ID|REPLY_ID):[^\]]*\]|\|?\s*TYPE:\s*\w+\s*\|?'
        r'|\|?\s*CHANNEL:\s*\w+\s*\|?|\|?\s*Source\s*:[^|]*\|?',
        '', text
    )
    return cleaned.strip(' |')


def compute_rag_metrics(content):
    """MÃ©triques spÃ©cifiques Ã  la qualitÃ© RAG d'un chunk"""
    if not content:
        return {
            'content_length': 0,
            'word_count': 0,
            'has_url_only': False,
            'is_question': False,
            'info_density': 0.0
        }

    words = content.split()
    word_count = len(words)

    # Un chunk qui n'est QU'une URL = inutile pour le RAG sÃ©mantique
    url_pattern = r'^https?://\S+$'
    has_url_only = bool(re.match(url_pattern, content.strip()))

    # Les questions courtes (replies) ont peu de valeur informationnelle
    is_question = content.strip().endswith('?') and word_count < 15

    # DensitÃ© informationnelle = ratio de mots "uniques" vs total
    unique_ratio = len(set(words)) / max(word_count, 1)

    # Score composite de qualitÃ© pour le RAG (0 Ã  1)
    density = 0.0
    if has_url_only:
        density = 0.1
    elif is_question and word_count < 8:
        density = 0.15
    else:
        length_score = min(word_count / 50, 1.0)  # Optimal ~50+ mots
        density = round(unique_ratio * 0.6 + length_score * 0.4, 3)

    return {
        'content_length': len(content),
        'word_count': word_count,
        'has_url_only': has_url_only,
        'is_question': is_question,
        'info_density': density
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 3 : CONSTRUCTION DU DATAFRAME ENRICHI
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

rows = []
contents_for_analysis = []  # Textes nettoyÃ©s pour l'analyse sÃ©mantique

for d in data:
    meta = d.get('metadata', {})
    text = d.get('text', '')

    doc_type = parse_doc_type(text, meta)
    content = extract_content(text)
    rag_metrics = compute_rag_metrics(content)

    channel = str(
        meta.get('channel id')
        or meta.get('channel_id')
        or meta.get('channel_name')
        or 'Unknown'
    )

    # Date parsing robuste
    date_raw = meta.get('date', '')
    try:
        date_parsed = pd.to_datetime(date_raw)
    except Exception:
        date_parsed = pd.NaT

    rows.append({
        'category':      meta.get('category', 'Unknown'),
        'doc_type':      doc_type,
        'channel':       channel,
        'date':          date_parsed,
        'views':         meta.get('views'),
        'forwards':      meta.get('forwards', 0),
        'is_recovered':  meta.get('recovered', False),
        'raw_text_len':  len(text),
        'content':       content,
        **rag_metrics  # content_length, word_count, has_url_only, etc.
    })

    if content and not rag_metrics['has_url_only']:
        contents_for_analysis.append(content.lower())

df = pd.DataFrame(rows)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 4 : ANALYSE DE QUALITÃ‰ RAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "=" * 50)
print("     ğŸ“Š RAPPORT DE QUALITÃ‰ RAG")
print("=" * 50)

total = len(df)
print(f"\nğŸ”¹ Total Documents       : {total}")
print(f"ğŸ”¹ Posts Originaux       : {len(df[df['doc_type'] == 'original_post'])}")
print(f"ğŸ”¹ RÃ©ponses (Replies)    : {len(df[df['doc_type'] == 'reply'])}")
print(f"ğŸ”¹ Recovered             : {df['is_recovered'].sum()}")

# --- MÃ©triques critiques pour le RAG ---
url_only = df['has_url_only'].sum()
questions_low = df['is_question'].sum()
empty = df[df['content_length'] == 0].shape[0]
low_density = df[df['info_density'] < 0.2].shape[0]
good_chunks = df[df['info_density'] >= 0.5].shape[0]

print(f"\n{'â”€'*40}")
print(f"  ğŸ¯ MÃ‰TRIQUES RAG")
print(f"{'â”€'*40}")
print(f"  âœ… Chunks exploitables (densitÃ©â‰¥0.5) : {good_chunks} ({100*good_chunks/total:.1f}%)")
print(f"  âš ï¸  URL seule (inutile sÃ©mantique)   : {url_only} ({100*url_only/total:.1f}%)")
print(f"  âš ï¸  Questions courtes (<15 mots)      : {questions_low}")
print(f"  âŒ Contenu vide                       : {empty}")
print(f"  âŒ DensitÃ© faible (<0.2)              : {low_density} ({100*low_density/total:.1f}%)")
print(f"  ğŸ“ Longueur moyenne du contenu        : {df['content_length'].mean():.0f} car.")
print(f"  ğŸ“ Mots moyens par chunk              : {df['word_count'].mean():.1f}")

# Distribution de la densitÃ© par catÃ©gorie
print(f"\n{'â”€'*40}")
print(f"  ğŸ“¦ DENSITÃ‰ MOYENNE PAR CATÃ‰GORIE")
print(f"{'â”€'*40}")
density_by_cat = (
    df.groupby('category')['info_density']
    .agg(['mean', 'count'])
    .sort_values('mean', ascending=False)
)
for cat, row in density_by_cat.iterrows():
    bar = "â–ˆ" * int(row['mean'] * 20)
    print(f"  {cat[:30].ljust(30)} : {row['mean']:.3f} {bar} (n={int(row['count'])})")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 5 : ANALYSE LEXICALE NETTOYÃ‰E
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

text_blob = " ".join(contents_for_analysis)

stop_words = {
    # English
    'the', 'and', 'for', 'this', 'that', 'with', 'from', 'your', 'all',
    'have', 'was', 'not', 'are', 'been', 'were', 'their', 'which', 'what',
    'when', 'where', 'who', 'will', 'would', 'should', 'can', 'could',
    'about', 'than', 'then', 'them', 'just', 'also', 'more', 'some',
    'into', 'only', 'very', 'here', 'there', 'every', 'each', 'much',
    # French
    'les', 'des', 'une', 'pour', 'dans', 'sur', 'est', 'aux', 'pas',
    'que', 'qui', 'avec', 'plus', 'vous', 'nous', 'sont', 'tout',
    'person',
    # Structural / RAG noise
    'type', 'content', 'channel', 'main_post', 'post', 'reply', 'source',
    'message', 'original', 'statut', 'accessible', 'restreint', 'none',
    'false', 'true', 'unknown', 'null', 'date_time', 'avertissement',
    # Web
    'https', 'http', 'link', 'download', 'file', 'files', 'click',
    'join', 'group', 'telegram', 'photo', 'media', 'video',
}

# Ajout dynamique des noms de canaux
stop_words.update(df['channel'].str.lower().unique())
stop_words.update(df['category'].str.lower().unique())

words = re.findall(r'\b[a-z]{4,}\b', text_blob)
filtered = [w for w in words if w not in stop_words]
kw_counts = Counter(filtered)

print(f"\n{'â”€'*40}")
print(f"  ğŸ”‘ TOP 20 MOTS-CLÃ‰S CTI")
print(f"{'â”€'*40}")
for kw, count in kw_counts.most_common(20):
    bar = "â–ˆ" * min(int(count / max(kw_counts.most_common(1)[0][1], 1) * 30), 30)
    print(f"  {kw.ljust(18)} : {str(count).rjust(5)} {bar}")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 6 : DASHBOARD VISUEL
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

sns.set_theme(style="whitegrid")
fig, axes = plt.subplots(2, 3, figsize=(24, 14))
plt.subplots_adjust(hspace=0.4, wspace=0.35)
fig.suptitle("ğŸ›¡ï¸ CTI RAG Quality Dashboard", fontsize=18, fontweight='bold', y=0.98)

# 1. RÃ©partition des catÃ©gories
axes[0, 0].set_title("ğŸ“¦ RÃ©partition des Menaces", fontsize=13, fontweight='bold')
cat_counts = df['category'].value_counts()
cat_counts.plot.pie(autopct='%1.1f%%', ax=axes[0, 0], colors=sns.color_palette("viridis", len(cat_counts)))
axes[0, 0].set_ylabel('')

# 2. Top canaux
axes[0, 1].set_title("ğŸ“¢ Top 10 Canaux Actifs", fontsize=13, fontweight='bold')
top_ch = df['channel'].value_counts().nlargest(10)
sns.barplot(x=top_ch.values, y=top_ch.index, ax=axes[0, 1], palette="magma")

# 3. â­ NOUVEAU : Distribution de la densitÃ© RAG
axes[0, 2].set_title("ğŸ¯ Distribution DensitÃ© RAG", fontsize=13, fontweight='bold')
df['info_density'].hist(bins=30, ax=axes[0, 2], color='teal', edgecolor='white', alpha=0.85)
axes[0, 2].axvline(x=0.5, color='red', linestyle='--', label='Seuil qualitÃ© (0.5)')
axes[0, 2].axvline(x=0.2, color='orange', linestyle='--', label='Seuil faible (0.2)')
axes[0, 2].legend()
axes[0, 2].set_xlabel('Info Density Score')

# 4. Types de documents
axes[1, 0].set_title("ğŸ“Š Types de Documents", fontsize=13, fontweight='bold')
type_counts = df['doc_type'].value_counts()
sns.barplot(x=type_counts.index, y=type_counts.values, ax=axes[1, 0], palette="coolwarm")

# 5. â­ NOUVEAU : QualitÃ© par catÃ©gorie (boxplot)
axes[1, 1].set_title("ğŸ“¦ DensitÃ© RAG par CatÃ©gorie", fontsize=13, fontweight='bold')
top_cats = df['category'].value_counts().nlargest(8).index
df_filtered = df[df['category'].isin(top_cats)]
sns.boxplot(data=df_filtered, y='category', x='info_density', ax=axes[1, 1], palette="Set2")
axes[1, 1].axvline(x=0.5, color='red', linestyle='--', alpha=0.5)

# 6. Top mots-clÃ©s
axes[1, 2].set_title("ğŸ”‘ Top 10 Mots-clÃ©s CTI", fontsize=13, fontweight='bold')
top_kw = dict(kw_counts.most_common(10))
sns.barplot(x=list(top_kw.values()), y=list(top_kw.keys()), ax=axes[1, 2], color="teal")

plt.savefig('cti_rag_dashboard.png', dpi=300, bbox_inches='tight')
print("\nâœ… Dashboard sauvegardÃ© : 'cti_rag_dashboard.png'")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 7 : RECOMMANDATIONS AUTOMATIQUES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print(f"\n{'â•'*50}")
print(f"  ğŸ’¡ RECOMMANDATIONS POUR TON PIPELINE RAG")
print(f"{'â•'*50}")

if url_only / total > 0.1:
    print(f"  âš ï¸  {url_only} docs sont des URL seules â†’ Enrichis-les")
    print(f"     (scrape le contenu) ou exclus-les de l'index")

if low_density / total > 0.3:
    print(f"  âš ï¸  {100*low_density/total:.0f}% de chunks ont une densitÃ©")
    print(f"     faible â†’ Fusionne les replies avec leur parent post")

if df['word_count'].median() < 20:
    print(f"  âš ï¸  MÃ©diane de {df['word_count'].median():.0f} mots/chunk")
    print(f"     â†’ Chunks trop courts, combine parent + replies")

if good_chunks / total < 0.5:
    print(f"  ğŸ”´ Seulement {100*good_chunks/total:.0f}% de chunks de qualitÃ©")
    print(f"     â†’ StratÃ©gie de chunking Ã  revoir")
else:
    print(f"  âœ… {100*good_chunks/total:.0f}% de chunks exploitables")
    print(f"     â†’ Base correcte pour le RAG")

print(f"\n{'â•'*50}\n")